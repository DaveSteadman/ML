{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\007-anaconda\\envs\\tensorflow_gpu\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# filepaths\n",
    "from os.path import basename\n",
    "\n",
    "# Regular expressions\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# main modelling\n",
    "import multiprocessing\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "# Split Train/Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# debug time output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\007-anaconda\\envs\\tensorflow_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential \n",
    "from keras.layers.recurrent import LSTM, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Keras Version: 2.1.6\n",
      "Keras Backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Keras Version:\", keras.__version__)\n",
    "print(\"Keras Backend:\", keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Sentence Text Processing\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def legal_char_for_char(c):\n",
    "    if   c == '\\n': return ' '\n",
    "    return c\n",
    "\n",
    "def sentence_to_wordlist(sent):\n",
    "    cleansent = re.sub(\"[^a-zA-Z]\",\" \", sent)\n",
    "    words = cleansent.split()\n",
    "    return words\n",
    "\n",
    "def sentence_end_char(c):\n",
    "    if   c == '.':  return True\n",
    "    elif c == '?':  return True\n",
    "    elif c == '!':  return True\n",
    "    return False\n",
    "\n",
    "def mid_sentence_abbr(str):\n",
    "    if   str.endswith(\"Mr.\"): return True\n",
    "    elif str.endswith(\"Mrs.\"): return True\n",
    "    elif str.endswith(\"Dr.\"): return True\n",
    "    elif str.endswith(\"Hon.\"): return True\n",
    "    elif str.endswith(\"etc.\"): return True\n",
    "    elif str.endswith(\"ie.\"): return True\n",
    "    elif str.endswith(\"AM.\"): return True\n",
    "    elif str.endswith(\"PM.\"): return True\n",
    "    return False\n",
    "\n",
    "def import_textfile_to_sentencelist(filepath):\n",
    "    accumulated_sentence = \"\"\n",
    "    sentencelist = []\n",
    "    c = ' '\n",
    "    with open (filepath, \"r\") as f:\n",
    "        while c: \n",
    "            c = f.read(1)\n",
    "            accumulated_sentence += legal_char_for_char(c)\n",
    "            \n",
    "            if ( (sentence_end_char(c)) and not (mid_sentence_abbr(accumulated_sentence)) ):\n",
    "                sentencelist.append(accumulated_sentence.strip())\n",
    "                accumulated_sentence = \"\"\n",
    "\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts on lines worth of word tokens to a sized list, with UNK padding\n",
    "\n",
    "def linelist_to_sizedunkwordlist(inputlinelist, sizelimit):\n",
    "    currlinelen = len(inputlinelist)\n",
    "    if currlinelen < sizelimit:\n",
    "        outputline = inputlinelist\n",
    "        for i in range (sizelimit - currlinelen):\n",
    "            outputline.append('unk')\n",
    "    if currlinelen > WordPerSentenceLimit:\n",
    "        outputline = currline[0:sizelimit-1]\n",
    "    return outputline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = w2v.Word2Vec.load('fullw2v_d250_v85000.w2v')\n",
    "w2vDimension = 250\n",
    "unk_vect     = w2v_model['unk']\n",
    "unk2_vect    = np.ones(w2vDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've found simple inputs, something like just the dialog_esl_conversations file, and around 10,000 epochs \n",
    "# of training, will start to give reasonable responses.\n",
    "\n",
    "model = load_model('bot_E100.h5')\n",
    "WordPerSentenceLimit = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter text:Hello\n",
      "['Hello']\n",
      "['Hello', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk']\n",
      "[('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552)]\n",
      "('that', 0.5332918763160706)\n",
      "('acuity', 0.5318058133125305)\n",
      "('beltane', 0.547277569770813)\n",
      "('beltane', 0.5536845922470093)\n",
      "('beltane', 0.5425119996070862)\n",
      "('unk', 0.47089889645576477)\n",
      "('unk', 0.6404502391815186)\n",
      "('unk', 0.6822835206985474)\n",
      "('unk', 0.6904045343399048)\n",
      "('unk', 0.6917603015899658)\n",
      "('unk', 0.6920086741447449)\n",
      "('unk', 0.6891658306121826)\n",
      "enter text:hello\n",
      "['hello']\n",
      "['hello', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk']\n",
      "[('hello', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552), ('unk', 0.9999999403953552)]\n",
      "('beltane', 0.5404849648475647)\n",
      "('beltane', 0.5475960969924927)\n",
      "('findings', 0.4745092988014221)\n",
      "('unk', 0.6413319706916809)\n",
      "('unk', 0.6808029413223267)\n",
      "('unk', 0.6897808313369751)\n",
      "('unk', 0.6919779777526855)\n",
      "('unk', 0.6927933096885681)\n",
      "('unk', 0.6932364106178284)\n",
      "('unk', 0.693381130695343)\n",
      "('unk', 0.6933245062828064)\n",
      "('unk', 0.689440131187439)\n",
      "enter text:\n",
      "Line too short\n"
     ]
    }
   ],
   "source": [
    "# To interpret the processing going on here, disect each output-line in turn.\n",
    "# 1 - The entered text\n",
    "# 2 - Cropped entered text\n",
    "# 3 - The w2v lookup of input text. If things are completely turned to unk's then check captialsation or the w2v contents.\n",
    "# 4 - The output and confidence value.\n",
    "# Enter a blank line to break the loop.\n",
    "\n",
    "running = True\n",
    "\n",
    "while (running):\n",
    "    \n",
    "    # Get a new line of text to process\n",
    "    newinput_line = input(\"enter text:\")\n",
    "    \n",
    "    # Exit if its too short to be valid\n",
    "    if len(newinput_line) < 2:\n",
    "        print(\"Line too short\")\n",
    "        break\n",
    "            \n",
    "    # split the new input into tokens\n",
    "    newinput_words = sentence_to_wordlist(newinput_line)\n",
    "    print(newinput_words)\n",
    "\n",
    "    # Size the wordlist for the model\n",
    "    sized_wordlist = linelist_to_sizedunkwordlist(newinput_words, WordPerSentenceLimit)\n",
    "    print(sized_wordlist)\n",
    "    \n",
    "    # create specifically sized array to populate and pass into model.\n",
    "    q_np_vect = np.zeros( (1, WordPerSentenceLimit, w2vDimension) )\n",
    "    \n",
    "    # Convert the words to vectors\n",
    "    for currwordindex in range(len(sized_wordlist)):\n",
    "        currword = sized_wordlist[currwordindex]\n",
    "        if (currword in w2v_model):\n",
    "            currvect = w2v_model[currword]\n",
    "        else:\n",
    "            currvect = unk_vect\n",
    "        for vectindex in range(w2vDimension):\n",
    "            q_np_vect[0][currwordindex][vectindex] = currvect[vectindex]\n",
    "    \n",
    "    # undo assignment to check inputs\n",
    "    inputcheck = []\n",
    "    for currwordindex in range(WordPerSentenceLimit):\n",
    "        matchlist = w2v_model.most_similar( [q_np_vect[0][currwordindex]] )\n",
    "        inputcheck.append(matchlist[0])\n",
    "    print (inputcheck)\n",
    "    \n",
    "    # Call model to perform predictions\n",
    "    predictions = model.predict(q_np_vect)\n",
    "    \n",
    "    # convert returned array of vectors back into closest words\n",
    "    for currwordindex in range(WordPerSentenceLimit):\n",
    "        matchlist = w2v_model.most_similar( [predictions[0][currwordindex]] )\n",
    "        print (matchlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
